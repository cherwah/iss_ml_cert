{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cultivar</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cultivar  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0           1    14.23        1.71  2.43               15.6        127   \n",
       "1           1    13.20        1.78  2.14               11.2        100   \n",
       "2           1    13.16        2.36  2.67               18.6        101   \n",
       "3           1    14.37        1.95  2.50               16.8        113   \n",
       "4           1    13.24        2.59  2.87               21.0        118   \n",
       "..        ...      ...         ...   ...                ...        ...   \n",
       "173         3    13.71        5.65  2.45               20.5         95   \n",
       "174         3    13.40        3.91  2.48               23.0        102   \n",
       "175         3    13.27        4.28  2.26               20.0        120   \n",
       "176         3    13.17        2.59  2.37               20.0        120   \n",
       "177         3    14.13        4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.80        3.06                  0.28             2.29   \n",
       "1             2.65        2.76                  0.26             1.28   \n",
       "2             2.80        3.24                  0.30             2.81   \n",
       "3             3.85        3.49                  0.24             2.18   \n",
       "4             2.80        2.69                  0.39             1.82   \n",
       "..             ...         ...                   ...              ...   \n",
       "173           1.68        0.61                  0.52             1.06   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "175           1.59        0.69                  0.43             1.35   \n",
       "176           1.65        0.68                  0.53             1.46   \n",
       "177           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0               5.64  1.04                          3.92     1065  \n",
       "1               4.38  1.05                          3.40     1050  \n",
       "2               5.68  1.03                          3.17     1185  \n",
       "3               7.80  0.86                          3.45     1480  \n",
       "4               4.32  1.04                          2.93      735  \n",
       "..               ...   ...                           ...      ...  \n",
       "173             7.70  0.64                          1.74      740  \n",
       "174             7.30  0.70                          1.56      750  \n",
       "175            10.20  0.59                          1.56      835  \n",
       "176             9.30  0.60                          1.62      840  \n",
       "177             9.20  0.61                          1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0      14.23        1.71  2.43               15.6        127           2.80   \n",
       "1      13.20        1.78  2.14               11.2        100           2.65   \n",
       "2      13.16        2.36  2.67               18.6        101           2.80   \n",
       "3      14.37        1.95  2.50               16.8        113           3.85   \n",
       "4      13.24        2.59  2.87               21.0        118           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5         95           1.68   \n",
       "174    13.40        3.91  2.48               23.0        102           1.80   \n",
       "175    13.27        4.28  2.26               20.0        120           1.59   \n",
       "176    13.17        2.59  2.37               20.0        120           1.65   \n",
       "177    14.13        4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     OD280/OD315 of diluted wines  Proline  \n",
       "0                            3.92     1065  \n",
       "1                            3.40     1050  \n",
       "2                            3.17     1185  \n",
       "3                            3.45     1480  \n",
       "4                            2.93      735  \n",
       "..                            ...      ...  \n",
       "173                          1.74      740  \n",
       "174                          1.56      750  \n",
       "175                          1.56      835  \n",
       "176                          1.62      840  \n",
       "177                          1.60      560  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = df.loc[:, 'Alcohol':]\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cultivar\n",
       "0           1\n",
       "1           1\n",
       "2           1\n",
       "3           1\n",
       "4           1\n",
       "..        ...\n",
       "173         3\n",
       "174         3\n",
       "175         3\n",
       "176         3\n",
       "177         3\n",
       "\n",
       "[178 rows x 1 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = df[['Cultivar']]\n",
    "\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.iloc[:,0].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "\tdf_features.values, df_label.values, test_size=0.1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appy standardization to our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appy PCA for dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance:  [0.36209939 0.18894383 0.11385758 0.07110837 0.06686914]\n",
      "explained variance:  0.8028783015869747\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(x_train)\n",
    "\n",
    "print('explained variance: ', pca.explained_variance_ratio_)\n",
    "print('explained variance: ', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform both our Training and Test data using the fitted PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-label our labels to start from 0 (required by to_categorical() function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print our labels into encoded categorical numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1 2 1 1 2 0 1 0 0 1 2 0 2 1 0 0 1 1 2 0 0 0 1 0 1 0 2 1 2 0 2 1 2 1\n",
      " 0 2 0 1 1 1 2 2 1 0 1 0 0 1 1 2 2 0 0 1 0 2 2 1 0 0 0 0 1 1 0 0 2 1 2 0 2\n",
      " 2 2 1 1 1 1 2 2 2 2 2 0 1 1 2 2 0 0 2 2 1 1 1 2 2 0 0 1 0 0 0 0 1 1 2 1 1\n",
      " 0 1 1 0 1 1 1 0 1 2 0 1 2 0 1 2 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0\n",
      " 1 2 1 2 1 1 2 1 2 0 2 1]\n",
      "[1 1 0 0 1 2 0 2 2 2 2 1 0 1 0 1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cherwah/python37_venv/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "y_train_le = le.transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "\n",
    "print(y_train_le)\n",
    "print(y_test_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform One-Hot Encoding on our categorical numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_1hot = tf.keras.utils.to_categorical(y_train_le, 3)\n",
    "y_test_1hot = tf.keras.utils.to_categorical(y_test_le, 3)\n",
    "\n",
    "print(y_train_1hot)\n",
    "print(y_test_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples\n",
      "Epoch 1/300\n",
      "160/160 [==============================] - 0s 165us/sample - loss: 0.9572 - acc: 0.5125\n",
      "Epoch 2/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.8400 - acc: 0.7750\n",
      "Epoch 3/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.7337 - acc: 0.8813\n",
      "Epoch 4/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.6454 - acc: 0.9062\n",
      "Epoch 5/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.5672 - acc: 0.9187\n",
      "Epoch 6/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.5026 - acc: 0.9375\n",
      "Epoch 7/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.4485 - acc: 0.9375\n",
      "Epoch 8/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.4022 - acc: 0.9438\n",
      "Epoch 9/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.3637 - acc: 0.9375\n",
      "Epoch 10/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.3311 - acc: 0.9500\n",
      "Epoch 11/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.3028 - acc: 0.9625\n",
      "Epoch 12/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.2791 - acc: 0.9750\n",
      "Epoch 13/300\n",
      "160/160 [==============================] - 0s 27us/sample - loss: 0.2585 - acc: 0.9750\n",
      "Epoch 14/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.2402 - acc: 0.9750\n",
      "Epoch 15/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.2247 - acc: 0.9750\n",
      "Epoch 16/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.2109 - acc: 0.9750\n",
      "Epoch 17/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.1983 - acc: 0.9750\n",
      "Epoch 18/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.1873 - acc: 0.9812\n",
      "Epoch 19/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1776 - acc: 0.9812\n",
      "Epoch 20/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.1687 - acc: 0.9812\n",
      "Epoch 21/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.1607 - acc: 0.9812\n",
      "Epoch 22/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1535 - acc: 0.9812\n",
      "Epoch 23/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1468 - acc: 0.9812\n",
      "Epoch 24/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1406 - acc: 0.9875\n",
      "Epoch 25/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.1351 - acc: 0.9875\n",
      "Epoch 26/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.1298 - acc: 0.9875\n",
      "Epoch 27/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.1250 - acc: 0.9937\n",
      "Epoch 28/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1206 - acc: 0.9937\n",
      "Epoch 29/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.1163 - acc: 0.9937\n",
      "Epoch 30/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.1125 - acc: 0.9937\n",
      "Epoch 31/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.1088 - acc: 0.9937\n",
      "Epoch 32/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1055 - acc: 0.9937\n",
      "Epoch 33/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.1023 - acc: 0.9937\n",
      "Epoch 34/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0993 - acc: 0.9937\n",
      "Epoch 35/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0963 - acc: 0.9937\n",
      "Epoch 36/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0936 - acc: 0.9937\n",
      "Epoch 37/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0910 - acc: 0.9937\n",
      "Epoch 38/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0886 - acc: 0.9937\n",
      "Epoch 39/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0862 - acc: 0.9937\n",
      "Epoch 40/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0842 - acc: 0.9937\n",
      "Epoch 41/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0819 - acc: 0.9937\n",
      "Epoch 42/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0800 - acc: 0.9937\n",
      "Epoch 43/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0782 - acc: 0.9937\n",
      "Epoch 44/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0762 - acc: 0.9937\n",
      "Epoch 45/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0746 - acc: 0.9937\n",
      "Epoch 46/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0729 - acc: 0.9937\n",
      "Epoch 47/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0711 - acc: 0.9937\n",
      "Epoch 48/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0696 - acc: 0.9937\n",
      "Epoch 49/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0682 - acc: 0.9937\n",
      "Epoch 50/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0667 - acc: 0.9937\n",
      "Epoch 51/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0652 - acc: 0.9937\n",
      "Epoch 52/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0640 - acc: 0.9937\n",
      "Epoch 53/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0626 - acc: 0.9937\n",
      "Epoch 54/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0613 - acc: 0.9937\n",
      "Epoch 55/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0601 - acc: 0.9937\n",
      "Epoch 56/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0590 - acc: 0.9937\n",
      "Epoch 57/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0578 - acc: 0.9937\n",
      "Epoch 58/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0567 - acc: 0.9937\n",
      "Epoch 59/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0556 - acc: 0.9937\n",
      "Epoch 60/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0546 - acc: 0.9937\n",
      "Epoch 61/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0535 - acc: 0.9937\n",
      "Epoch 62/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0527 - acc: 0.9937\n",
      "Epoch 63/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0516 - acc: 0.9937\n",
      "Epoch 64/300\n",
      "160/160 [==============================] - 0s 30us/sample - loss: 0.0506 - acc: 0.9937\n",
      "Epoch 65/300\n",
      "160/160 [==============================] - 0s 32us/sample - loss: 0.0497 - acc: 0.9937\n",
      "Epoch 66/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0488 - acc: 0.9937\n",
      "Epoch 67/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0480 - acc: 0.9937\n",
      "Epoch 68/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0472 - acc: 0.9937\n",
      "Epoch 69/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0464 - acc: 0.9937\n",
      "Epoch 70/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0456 - acc: 0.9937\n",
      "Epoch 71/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0447 - acc: 0.9937\n",
      "Epoch 72/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0440 - acc: 0.9937\n",
      "Epoch 73/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0434 - acc: 0.9937\n",
      "Epoch 74/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0427 - acc: 0.9937\n",
      "Epoch 75/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0419 - acc: 0.9937\n",
      "Epoch 76/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0412 - acc: 0.9937\n",
      "Epoch 77/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0406 - acc: 0.9937\n",
      "Epoch 78/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0399 - acc: 0.9937\n",
      "Epoch 79/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0393 - acc: 0.9937\n",
      "Epoch 80/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0387 - acc: 0.9937\n",
      "Epoch 81/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0381 - acc: 0.9937\n",
      "Epoch 82/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0375 - acc: 0.9937\n",
      "Epoch 83/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0368 - acc: 0.9937\n",
      "Epoch 84/300\n",
      "160/160 [==============================] - 0s 17us/sample - loss: 0.0364 - acc: 0.9937\n",
      "Epoch 85/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 86/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0353 - acc: 1.0000\n",
      "Epoch 87/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0348 - acc: 1.0000\n",
      "Epoch 88/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0343 - acc: 1.0000\n",
      "Epoch 89/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0338 - acc: 1.0000\n",
      "Epoch 90/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 91/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0327 - acc: 1.0000\n",
      "Epoch 92/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0323 - acc: 1.0000\n",
      "Epoch 93/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0319 - acc: 1.0000\n",
      "Epoch 94/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0313 - acc: 1.0000\n",
      "Epoch 95/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 96/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 97/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 98/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 99/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0293 - acc: 1.0000\n",
      "Epoch 100/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 101/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0286 - acc: 1.0000\n",
      "Epoch 102/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 103/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 104/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 105/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 106/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0266 - acc: 1.0000\n",
      "Epoch 107/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0263 - acc: 1.0000\n",
      "Epoch 108/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0259 - acc: 1.0000\n",
      "Epoch 109/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 110/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 111/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0250 - acc: 1.0000\n",
      "Epoch 112/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 113/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 114/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0240 - acc: 1.0000\n",
      "Epoch 115/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0236 - acc: 1.0000\n",
      "Epoch 116/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0233 - acc: 1.0000\n",
      "Epoch 117/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 118/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 119/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0225 - acc: 1.0000\n",
      "Epoch 120/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 121/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 122/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 123/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0214 - acc: 1.0000\n",
      "Epoch 124/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 125/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 126/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 127/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 128/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 129/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 130/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 131/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 132/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 133/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 134/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 135/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 136/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 137/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 138/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 139/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 140/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 141/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 142/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 143/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 144/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 145/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 146/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 147/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 148/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 149/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 150/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 151/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 152/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 153/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 154/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 155/300\n",
      "160/160 [==============================] - 0s 54us/sample - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 156/300\n",
      "160/160 [==============================] - 0s 159us/sample - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 157/300\n",
      "160/160 [==============================] - 0s 43us/sample - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 158/300\n",
      "160/160 [==============================] - 0s 40us/sample - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 159/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0141 - acc: 1.0000\n",
      "Epoch 160/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 161/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 162/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 163/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 164/300\n",
      "160/160 [==============================] - 0s 29us/sample - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 165/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 166/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 167/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 168/300\n",
      "160/160 [==============================] - 0s 27us/sample - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 169/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 170/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 171/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 172/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 173/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 174/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 175/300\n",
      "160/160 [==============================] - 0s 62us/sample - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 176/300\n",
      "160/160 [==============================] - 0s 36us/sample - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 177/300\n",
      "160/160 [==============================] - 0s 30us/sample - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 178/300\n",
      "160/160 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 179/300\n",
      "160/160 [==============================] - 0s 34us/sample - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 180/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 181/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 182/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 183/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0110 - acc: 1.0000\n",
      "Epoch 184/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 185/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 186/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 187/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 188/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 189/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 190/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 191/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 192/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 193/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 194/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 195/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 196/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 197/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 198/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 199/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 200/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 201/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 202/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 203/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 204/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 205/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 206/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 207/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 208/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 209/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 210/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 211/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 212/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 213/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 214/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 215/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 216/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 217/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 218/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 219/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 220/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 221/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 222/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 223/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 224/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 225/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 226/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 227/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 228/300\n",
      "160/160 [==============================] - 0s 27us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 229/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 230/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 231/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 232/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 233/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 234/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 235/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 236/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 237/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 238/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 239/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 240/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 241/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 242/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 243/300\n",
      "160/160 [==============================] - 0s 17us/sample - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 244/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 245/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 246/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 247/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 248/300\n",
      "160/160 [==============================] - 0s 27us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 249/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 250/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 251/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 252/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 253/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 254/300\n",
      "160/160 [==============================] - 0s 25us/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 255/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 256/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 257/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 258/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 259/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 260/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 261/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 262/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 263/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 264/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 265/300\n",
      "160/160 [==============================] - 0s 19us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 266/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 267/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 268/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 269/300\n",
      "160/160 [==============================] - 0s 18us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 270/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 271/300\n",
      "160/160 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 272/300\n",
      "160/160 [==============================] - 0s 34us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 273/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 274/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 275/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 276/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 277/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 278/300\n",
      "160/160 [==============================] - 0s 21us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 279/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 280/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 281/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 282/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 283/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 284/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 285/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 286/300\n",
      "160/160 [==============================] - 0s 52us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 287/300\n",
      "160/160 [==============================] - 0s 108us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 288/300\n",
      "160/160 [==============================] - 0s 151us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 289/300\n",
      "160/160 [==============================] - 0s 31us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 290/300\n",
      "160/160 [==============================] - 0s 26us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 291/300\n",
      "160/160 [==============================] - 0s 24us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 292/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 293/300\n",
      "160/160 [==============================] - 0s 28us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 294/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 295/300\n",
      "160/160 [==============================] - 0s 22us/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 296/300\n",
      "160/160 [==============================] - 0s 23us/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 297/300\n",
      "160/160 [==============================] - 0s 40us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 298/300\n",
      "160/160 [==============================] - 0s 63us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 299/300\n",
      "160/160 [==============================] - 0s 37us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 300/300\n",
      "160/160 [==============================] - 0s 20us/sample - loss: 0.0043 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train_1hot, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our \"loss\" and \"accuracy\" graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax[0].plot(history.history['loss'])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss Curve')\n",
    "\n",
    "ax[1].plot(history.history['acc'])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Accuracy Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform auto evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  0.00010824451601365581\n",
      "accuracy =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cherwah/python37_venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2366: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x=x_test, y=y_test_1hot)\n",
    "\n",
    "print('loss = ', loss)\n",
    "print('accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eye-ball predicted values vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  [0. 1. 0.] Predicted:  [7.8120478e-04 9.9921668e-01 2.1418382e-06]\n",
      "Actual:  [0. 1. 0.] Predicted:  [9.0354547e-08 9.9999988e-01 3.7968233e-09]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9998665e-01 1.5937830e-07 1.3250442e-05]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9997520e-01 1.4415181e-06 2.3415840e-05]\n",
      "Actual:  [0. 1. 0.] Predicted:  [2.9891915e-08 1.0000000e+00 1.4799814e-10]\n",
      "Actual:  [0. 0. 1.] Predicted:  [4.3782566e-05 1.2525100e-05 9.9994373e-01]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9999952e-01 8.6236076e-09 5.1774947e-07]\n",
      "Actual:  [0. 0. 1.] Predicted:  [5.5099315e-07 2.8834780e-08 9.9999940e-01]\n",
      "Actual:  [0. 0. 1.] Predicted:  [2.3547223e-07 2.2527107e-05 9.9997723e-01]\n",
      "Actual:  [0. 0. 1.] Predicted:  [4.5351298e-06 2.0273402e-04 9.9979275e-01]\n",
      "Actual:  [0. 0. 1.] Predicted:  [2.7487713e-06 7.0443207e-08 9.9999714e-01]\n",
      "Actual:  [0. 1. 0.] Predicted:  [6.0493226e-06 9.9999392e-01 4.2691610e-08]\n",
      "Actual:  [1. 0. 0.] Predicted:  [1.0000000e+00 7.0663454e-11 5.4270952e-08]\n",
      "Actual:  [0. 1. 0.] Predicted:  [1.2797906e-07 9.9999976e-01 1.2934694e-07]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9994576e-01 1.3983717e-07 5.4071221e-05]\n",
      "Actual:  [0. 1. 0.] Predicted:  [4.6858157e-05 9.9987435e-01 7.8789693e-05]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9935037e-01 4.8068506e-04 1.6893260e-04]\n",
      "Actual:  [0. 1. 0.] Predicted:  [2.8036416e-07 9.9999976e-01 2.0558359e-10]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x=x_test)\n",
    "for i in np.arange(len(predictions)):\n",
    "\tprint('Actual: ', y_test_1hot[i], 'Predicted: ', predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform manual calculation for accuracy in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 18, wrong: 0\n",
      "accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "n_preds = len(predictions)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for i in np.arange(n_preds):\n",
    "    pred_max = np.argmax(predictions[i])\n",
    "    actual_max = np.argmax(y_test_1hot[i])\n",
    "    if pred_max == actual_max:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print('correct: {0}, wrong: {1}'.format(correct, wrong))\n",
    "print('accuracy =', correct/n_preds)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c4aaadf913e60f2c06022d8c17ff2c7b144de1e1057a3cb7057cc7781dfc3c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('python37_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
