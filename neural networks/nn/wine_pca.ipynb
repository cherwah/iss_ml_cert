{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining a Wine's Cultivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cultivar</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cultivar  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0           1    14.23        1.71  2.43               15.6        127   \n",
       "1           1    13.20        1.78  2.14               11.2        100   \n",
       "2           1    13.16        2.36  2.67               18.6        101   \n",
       "3           1    14.37        1.95  2.50               16.8        113   \n",
       "4           1    13.24        2.59  2.87               21.0        118   \n",
       "..        ...      ...         ...   ...                ...        ...   \n",
       "173         3    13.71        5.65  2.45               20.5         95   \n",
       "174         3    13.40        3.91  2.48               23.0        102   \n",
       "175         3    13.27        4.28  2.26               20.0        120   \n",
       "176         3    13.17        2.59  2.37               20.0        120   \n",
       "177         3    14.13        4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.80        3.06                  0.28             2.29   \n",
       "1             2.65        2.76                  0.26             1.28   \n",
       "2             2.80        3.24                  0.30             2.81   \n",
       "3             3.85        3.49                  0.24             2.18   \n",
       "4             2.80        2.69                  0.39             1.82   \n",
       "..             ...         ...                   ...              ...   \n",
       "173           1.68        0.61                  0.52             1.06   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "175           1.59        0.69                  0.43             1.35   \n",
       "176           1.65        0.68                  0.53             1.46   \n",
       "177           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0               5.64  1.04                          3.92     1065  \n",
       "1               4.38  1.05                          3.40     1050  \n",
       "2               5.68  1.03                          3.17     1185  \n",
       "3               7.80  0.86                          3.45     1480  \n",
       "4               4.32  1.04                          2.93      735  \n",
       "..               ...   ...                           ...      ...  \n",
       "173             7.70  0.64                          1.74      740  \n",
       "174             7.30  0.70                          1.56      750  \n",
       "175            10.20  0.59                          1.56      835  \n",
       "176             9.30  0.60                          1.62      840  \n",
       "177             9.20  0.61                          1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset/wine.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0      14.23        1.71  2.43               15.6        127           2.80   \n",
       "1      13.20        1.78  2.14               11.2        100           2.65   \n",
       "2      13.16        2.36  2.67               18.6        101           2.80   \n",
       "3      14.37        1.95  2.50               16.8        113           3.85   \n",
       "4      13.24        2.59  2.87               21.0        118           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5         95           1.68   \n",
       "174    13.40        3.91  2.48               23.0        102           1.80   \n",
       "175    13.27        4.28  2.26               20.0        120           1.59   \n",
       "176    13.17        2.59  2.37               20.0        120           1.65   \n",
       "177    14.13        4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     OD280/OD315 of diluted wines  Proline  \n",
       "0                            3.92     1065  \n",
       "1                            3.40     1050  \n",
       "2                            3.17     1185  \n",
       "3                            3.45     1480  \n",
       "4                            2.93      735  \n",
       "..                            ...      ...  \n",
       "173                          1.74      740  \n",
       "174                          1.56      750  \n",
       "175                          1.56      835  \n",
       "176                          1.62      840  \n",
       "177                          1.60      560  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = df.loc[:, 'Alcohol':]\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cultivar\n",
       "0           1\n",
       "1           1\n",
       "2           1\n",
       "3           1\n",
       "4           1\n",
       "..        ...\n",
       "173         3\n",
       "174         3\n",
       "175         3\n",
       "176         3\n",
       "177         3\n",
       "\n",
       "[178 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = df[['Cultivar']]\n",
    "\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.iloc[:,0].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "\tdf_features.values, df_label.values, test_size=0.1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appy standardization to our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appy PCA for dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance:  [0.36302092 0.19152175 0.11522132 0.06902175 0.06354606]\n",
      "explained variance:  0.802331812693907\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(x_train)\n",
    "\n",
    "print('explained variance: ', pca.explained_variance_ratio_)\n",
    "print('explained variance: ', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform both our Training and Test data using the fitted PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-label our labels to start from 0 (required by to_categorical() function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print our labels into encoded categorical numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1 0 1 2 0 2 0 0 2 2 0 0 2 2 2 2 2 1 2 0 1 0 0 0 0 0 0 1 0 2 0 1 2 2\n",
      " 2 1 0 1 0 0 2 0 0 0 1 0 2 2 1 1 1 1 1 1 1 1 2 1 0 2 2 1 1 0 2 1 2 0 1 1 0\n",
      " 0 1 1 1 1 2 1 0 2 1 0 0 1 2 2 1 0 0 2 1 2 2 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 2 0 0 1 1 1 1 0 1 2 2 1 1 1 0 1 2 1 0 0 2 0 2 1 1 1 1 1 0 2 0 1 0 1 1 1 2\n",
      " 1 1 1 1 0 2 2 0 1 0 2 2]\n",
      "[2 1 0 2 1 1 0 0 1 0 1 2 1 0 0 2 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "y_train_le = le.transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "\n",
    "print(y_train_le)\n",
    "print(y_test_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform One-Hot Encoding on our categorical numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_1hot = tf.keras.utils.to_categorical(y_train_le, 3)\n",
    "y_test_1hot = tf.keras.utils.to_categorical(y_test_le, 3)\n",
    "\n",
    "print(y_train_1hot)\n",
    "print(y_test_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 1.3106 - accuracy: 0.1063\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.1716 - accuracy: 0.2000\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0490 - accuracy: 0.3063\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.9373 - accuracy: 0.6000\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8356 - accuracy: 0.7875\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7470 - accuracy: 0.8625\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6689 - accuracy: 0.9000\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6002 - accuracy: 0.9250\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5409 - accuracy: 0.9375\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.9438\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4427 - accuracy: 0.9500\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.9500\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.9500\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.9625\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.9688\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2875 - accuracy: 0.9688\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.9688\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.9688\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9688\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9688\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9750\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1930 - accuracy: 0.9750\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9812\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9812\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1644 - accuracy: 0.9812\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9812\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9812\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9875\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9875\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1313 - accuracy: 0.9875\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.9875\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9937\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9937\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.9937\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9937\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9937\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9937\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.9937\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0961 - accuracy: 0.9937\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9937\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0905 - accuracy: 0.9937\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9937\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9937\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9937\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0813 - accuracy: 0.9937\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9937\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9937\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9937\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9937\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9937\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9937\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9937\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9937\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9937\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9937\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9937\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9937\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9937\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9937\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9937\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9937\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9937\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9937\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9937\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9937\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0517 - accuracy: 0.9937\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9937\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 0.9937\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.9937\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9937\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9937\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9937\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9937\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9937\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0444 - accuracy: 0.9937\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9937\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.9937\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9937\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 245/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train_1hot, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our \"loss\" and \"accuracy\" graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax[0].plot(history.history['loss'])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss Curve')\n",
    "\n",
    "ax[1].plot(history.history['accuracy'])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Accuracy Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform auto evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "loss =  0.003180605825036764\n",
      "accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x=x_test, y=y_test_1hot)\n",
    "\n",
    "print('loss = ', loss)\n",
    "print('accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eye-ball predicted values vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  [0. 0. 1.] Predicted:  [8.9204323e-06 5.6174301e-08 9.9999106e-01]\n",
      "Actual:  [0. 1. 0.] Predicted:  [1.0539181e-08 1.0000000e+00 4.0870574e-08]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9949992e-01 3.7855943e-04 1.2149668e-04]\n",
      "Actual:  [0. 0. 1.] Predicted:  [5.8472287e-06 1.2320138e-06 9.9999297e-01]\n",
      "Actual:  [0. 1. 0.] Predicted:  [3.6910498e-03 9.9583250e-01 4.7649010e-04]\n",
      "Actual:  [0. 1. 0.] Predicted:  [1.5343769e-05 9.9990523e-01 7.9343561e-05]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9997282e-01 5.9586773e-06 2.1166425e-05]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9999976e-01 2.3573163e-09 2.3004640e-07]\n",
      "Actual:  [0. 1. 0.] Predicted:  [2.8646308e-10 1.0000000e+00 3.6140563e-10]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9998391e-01 4.9063315e-06 1.1198086e-05]\n",
      "Actual:  [0. 1. 0.] Predicted:  [4.6741959e-02 9.5321929e-01 3.8697326e-05]\n",
      "Actual:  [0. 0. 1.] Predicted:  [3.7841412e-05 4.0873657e-03 9.9587482e-01]\n",
      "Actual:  [0. 1. 0.] Predicted:  [3.9092672e-08 9.9999833e-01 1.6467069e-06]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9987483e-01 1.8062877e-06 1.2340683e-04]\n",
      "Actual:  [1. 0. 0.] Predicted:  [9.9999869e-01 3.4328366e-07 9.2193523e-07]\n",
      "Actual:  [0. 0. 1.] Predicted:  [2.3224025e-05 1.8923245e-04 9.9978751e-01]\n",
      "Actual:  [0. 1. 0.] Predicted:  [1.9315078e-06 9.9999785e-01 2.0423280e-07]\n",
      "Actual:  [0. 0. 1.] Predicted:  [1.1260754e-05 2.2162376e-05 9.9996662e-01]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x=x_test)\n",
    "for i in np.arange(len(predictions)):\n",
    "\tprint('Actual: ', y_test_1hot[i], 'Predicted: ', predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform manual calculation for accuracy in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 18, wrong: 0\n",
      "accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "n_preds = len(predictions)\n",
    "\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for i in np.arange(n_preds):\n",
    "    pred_max = np.argmax(predictions[i])\n",
    "    actual_max = np.argmax(y_test_1hot[i])\n",
    "    if pred_max == actual_max:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print('correct: {0}, wrong: {1}'.format(correct, wrong))\n",
    "print('accuracy =', correct/n_preds)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c4aaadf913e60f2c06022d8c17ff2c7b144de1e1057a3cb7057cc7781dfc3c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('python37_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
